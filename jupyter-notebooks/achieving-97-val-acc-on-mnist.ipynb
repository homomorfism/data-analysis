{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Домашнее задание:\n",
    "- Получить точность 97.5% на валидации MNIST.\n",
    "- Реализовать морфинг автоэнкодером (без формальных критериев — просто получите красивую гифку).\n",
    "- Визуализировать MNIST автоэнкодером (обучить автоэнкодер с латентным пространством размерности 2 и вывести через scatter точки разного цвета)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Линка на колаб с тетрадкой: [kaggle.com](https://www.kaggle.com/hashshes/training-mnist-97-val-accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If running on colab\n",
    "\n",
    "# ! pip install -qqq pytorch-lightning torch torchvision torchmetrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torchmetrics.classification import Accuracy\n",
    "\n",
    "pl.seed_everything(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size': 500,\n",
    "    'lr': 0.001,\n",
    "    'max_epochs': 100,\n",
    "    'lr_step': 30,\n",
    "    'weight_decay': 10\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining the dataloader for mnist model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "default_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class MNISTDataloader(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size: int):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dataset = datasets.MNIST(root=\"../data/raw\", download=True, train=True, transform=default_transform)\n",
    "        self.test_dataset = datasets.MNIST(root=\"../data/raw\", download=True, train=False, transform=default_transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(dataset=self.train_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(dataset=self.test_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining the dense model for mnist model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=28 * 28, out_features=100),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=100, out_features=10)\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining training loop for our model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class MNISTClassifier(pl.LightningModule):\n",
    "    def __init__(self, _model: nn.Module, _config: dict):\n",
    "        super().__init__()\n",
    "        self.model = _model\n",
    "        self.config = _config\n",
    "\n",
    "        self.train_accuracy = Accuracy()\n",
    "        self.val_accuracy = Accuracy()\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.count_epoch = 0\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        outputs = self.model(x)\n",
    "        loss = self.loss_fn(outputs, y)\n",
    "\n",
    "        self.log(\"train/loss_step\", loss.item())\n",
    "        self.log('train/acc_step', self.train_accuracy(outputs, y))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs) -> None:\n",
    "        self.count_epoch += 1\n",
    "        print(f'Train accuracy on {self.count_epoch} epoch: {self.train_accuracy.compute()}')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        outputs = self.model(x)\n",
    "        self.log('val/acc_step', self.val_accuracy(outputs, y))\n",
    "\n",
    "    def validation_epoch_end(self, outputs) -> None:\n",
    "        print(f\"Val accuracy on {self.count_epoch - 1} epoch: {self.val_accuracy.compute()}\")\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.config['lr'])\n",
    "        lr_scheduler = StepLR(optimizer=optimizer, step_size=self.config['lr_step'])\n",
    "\n",
    "        return [optimizer, ], [lr_scheduler, ]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/shamil/PycharmProjects/data-analysis-sklearn/venv/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "/home/shamil/PycharmProjects/data-analysis-sklearn/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:101: UserWarning: you defined a validation_step but have no val_dataloader. Skipping val loop\n",
      "  rank_zero_warn(f\"you defined a {step_name} but have no {loader_name}. Skipping {stage} loop\")\n",
      "\n",
      "  | Name           | Type             | Params\n",
      "----------------------------------------------------\n",
      "0 | model          | Sequential       | 79.5 K\n",
      "1 | train_accuracy | Accuracy         | 0     \n",
      "2 | val_accuracy   | Accuracy         | 0     \n",
      "3 | loss_fn        | CrossEntropyLoss | 0     \n",
      "----------------------------------------------------\n",
      "79.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "79.5 K    Total params\n",
      "0.318     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation sanity check: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7fa6e22527524c9c868650da685f2880"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: -1it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e806da97320d454d90b78f576816d2c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy on 1 epoch: 0.8823000192642212\n"
     ]
    }
   ],
   "source": [
    "def train(_config: dict):\n",
    "    trainer = pl.Trainer(gpus=0, max_epochs=_config['max_epochs'],)\n",
    "\n",
    "    data_loader = MNISTDataloader(batch_size=_config['batch_size'])\n",
    "    classifier = MNISTClassifier(_model=model, _config=_config)\n",
    "    trainer.fit(classifier, data_loader.train_dataloader(), data_loader.val_dataloader())\n",
    "    trainer.save_checkpoint(\"../weights/mnist.ckpt\", weights_only=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train(config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}